mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA"),
total_cases >= 100) %>%
select(date, continent, total_cases) %>%
pivot_longer(cols = c(total_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA"),
total_cases >= 100) %>%
select(date, continent, total_cases) %>%
pivot_longer(cols = c(total_cases)) %>%
arrange(date) %>%
ggplot()+
geom_line(aes(x = date, y = value, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA"),
total_cases >= 100) %>%
select(date, continent, total_cases) %>%
pivot_longer(cols = c(total_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA"),
total_cases >= 100) %>%
select(date, continent, total_cases) %>%
pivot_longer(cols = c(total_cases)) %>%
arrange(date) %>% View()
data %>%
ggplot()+
geom_line(aes(x = date, y = total_cases))
data %>%
ggplot()+
geom_point(aes(x = date, y = total_cases))
data %>%
filter(iso_code == "USA") %>%
ggplot()+
geom_point(aes(x = date, y = total_cases))
data %>%
filter(continent == "Europe") %>%
ggplot()+
geom_point(aes(x = date, y = total_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
group_by(continent) %>%
summarise(cases = sum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = total_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
group_by(continent) %>%
summarise(cases = sum(new_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continet %in% c("Europe", "USA")) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA")) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA")) %>%
group_by(continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA")) %>%
group_by(continent) %>%
summarise(cases = cumsum(new_cases))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA")) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_point(aes(x = date, y = cases, color = continent))
data %>%
mutate(continent = ifelse(iso_code == "USA", "USA", continent)) %>%
filter(continent %in% c("Europe", "USA")) %>%
group_by(date, continent) %>%
summarise(cases = cumsum(new_cases)) %>%
ggplot()+
geom_line(aes(x = date, y = cases, color = continent))
install.packages("GLMsData")
library(GLMsData)
data(trees)
cherry.m1 <- glm( Volume ~ log(Height) + log(Girth), data=trees,
family=Gamma(link="log"))
cherry.m1
summary(cherry.m1)
cherry.m1$data
(cherry.m1$data$Volume - cherry.m1$fitted.values)^2 / cherry.m1$fitted.values
cherry.m1
sum((cherry.m1$data$Volume - cherry.m1$fitted.values)^2 / cherry.m1$fitted.values)/28
summary(cherry.m1)
trees
cherry.m1 <- glm( Volume ~ log(Height) + log(Girth), data=trees,
family=poisson(link="log"))
trees$Volume <- round(trees$Volume)
trees$Volume
cherry.m1 <- glm( Volume ~ log(Height) + log(Girth), data=trees,
family=poisson(link="log"))
summary(cherry.m1)
cherry.m1 <- glm( Volume ~ log(Height) + log(Girth), data=trees,
family=quasipoisson(link="log"))
summary(cherry.m1)
sum((cherry.m1$data$Volume - cherry.m1$fitted.values)^2 / cherry.m1$fitted.values)/28
library(ChainLadder)
RAA %>%
cum2incr() %>%
as.data.frame()
library(magrittr)
RAA %>%
cum2incr() %>%
as.data.frame()
RAA %>%
cum2incr() %>%
as.data.frame() %>%
na.omit()
data <- RAA %>%
cum2incr() %>%
as.data.frame() %>%
na.omit()
rownames(data) <- NULL
data
source('~/.active-rstudio-document', echo=TRUE)
data$dev <- as.factor(data$dev)
model <- glm(value ~ origin + dev + 0, family = quasipoisson(link = "log"))
model <- glm(value ~ origin + dev + 0, family = quasipoisson(link = "log"), data = data)
library(ChainLadder)
library(magrittr)
data <- RAA %>%
cum2incr() %>%
as.data.frame() %>%
na.omit()
rownames(data) <- NULL
data$origin <- as.factor(data$origin)
data$dev <- as.factor(data$dev)
data
data <- ABC %>%
cum2incr() %>%
as.data.frame() %>%
na.omit()
rownames(data) <- NULL
data$origin <- as.factor(data$origin)
data$dev <- as.factor(data$dev)
model <- glm(value ~ origin + dev + 0, family = quasipoisson(link = "log"), data = data)
model
coefficients(model)
coef <- coefficients(model)
exp(coef["origin1977"] + coef["dev2"])
model$fitted.values
exo(coef["dev2"])
exp(coef["dev2"])
exp(coef["origin1977"]) * exp(coef["dev2"]))
exp(coef["origin1977"]) * exp(coef["dev2"])
exp(coef["origin1977"])
exp(coef["dev2"])
exp(coef["origin1977"])
exp(coef["origin1977"]) * exp(coef["dev3"])
exp(coef["origin1977"]) * exp(coef["dev2":"dev11"])
exp(coef["origin1977"]) * exp(coef[12:21])
model$fitted.values
sum(exp(coef[12:21]))
1/exp(coef[12:21])
sum(exp(coef[12:21]))
exp(sum(coef[12:21]))
exp(coef["origin1977"]) * exp(coef[12:21])
exp(coef["origin1977"]) * exp(coef[12:13])
exp(coef["origin1977"]) * exp(coef[12:13]) %>% sum()
exp(coef["origin1977"]) * exp(coef[12]+coef[13])
exp(coef["origin1977"]) * exp(coef[12]*coef[13])
exp(coef["origin1977"]) * exp(coef[12:13]) %>% sum()
exp(coef["origin1977"]) * exp(coef[12]*coef[13])
exp(coef["origin1977"]) * exp(coef[12] + coef[13])
exp(coef["origin1977"]) * exp(coef[12:13]) %>% sum()
exp(coef[12:13])
exp(coef[12:13])
exp(coef["origin1977"]) * exp(coef[12:13])
model$fitted.values
exp(coef[12:21])
exp(coef["origin1977"]) * sum(exp(coef[12:21]))
sum(exp(coef[12:21]))
exp(coef[12:13]) / sum(exp(coef[12:13]))
exp(coef[12:21]) / sum(exp(coef[12:21]))
sum(exp(coef[12:21]) / sum(exp(coef[12:21])))
sum(1, exp(coef[12:21]))
c(1, coef[12:21])
exp(c(0, coef[12:21]))
sum(1, exp(coef[12:21]))
exp(exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])))
sum(exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])))
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21]))
exp(coef["origin1977"])
exp(coef["origin1977"]) * sum(1, exp(coef[12:21]))
sum(1, exp(coef[12:21]))
exp(coef["origin1977"]) * sum(1, exp(coef[12:21])) * exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21]))
model$fitted.values
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21]))
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])) %>% plot(type = "l")
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21]))
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])) %>% plot()
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])) %>% as.vector()
exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])) %>% as.vector() %>% plot()
inc_perc <- exp(c(0, coef[12:21])) / sum(1, exp(coef[12:21])) %>% as.vector()
inc_perc
inc_perc %>% unname()
inc_perc %>% unname() %>% plot()
inc_perc %>% unname() %>% plot(type = "l")
inc_perc  %>% plot(type = "l")
inc_perc %>% plot(type = "l")
inc_perc %>% cumsum() %>% plot(type = "l")
inc_perc %>% plot(type = "l")
inc_perc %>% cumsum() %>% plot(type = "l")
inc_perc %>% plot(type = "l")
inc_perc %>% cumsum() %>% plot(type = "l")
exp(coef["origin1977"]) * sum(1, exp(coef[12:21]))
exp(coef[1:11]) * sum(1, exp(coef[12:21]))
ABC
coef[1:11]
coef[12:21]
coef[12:21] %>% diff()
c(coef[12], coef[12:21] %>% diff())
coef[12:21]
c(coef[12], coef[12:21] %>% diff())
0.2689569 -0.2972065
.2689569 -0.2972065 -0.3934492
library(ChainLadder)
triangle(c(75, 45, 15),
c(80, 35),
95)
data <- triangle(c(75, 45, 15),
c(80, 35),
95)
as.data.frame(data)
library(magrittr)
as.data.frame(data) %>%
na.omit()
data <- as.data.frame(data)
data <- triangle(c(75, 45, 15),
c(80, 35),
95)
data <- as.data.frame(data)
data$origin <- as.factor(data$origin)
data$dev <- as.factor(data$dev)
data$dev
data
model <- glm(value ~ origin + dev + 0, family = quasipoisson(link = "log"), data = data)
model$fitted.values
hatvalues(model)
resid( model, type="pearson" )
resid( model, type="pearson" ) * 1/sqrt(1 - hatvalues(model))
rstandard(model)
rstandard(model, type = "pearson")
summary(model)$dispersion
resid( model, type="pearson" ) sqrt(summary(model)$dispersion * (1 - hatvalues(model))
resid( model, type="pearson" ) / sqrt(summary(model)$dispersion * (1 - hatvalues(model))
resid( model, type="pearson" ) / sqrt(summary(model)$dispersion * (1 - hatvalues(model)))
summary(model)$dispersion
sqrt(summary(model)$dispersion * (1 - hatvalues(model))
sqrt(summary(model)$dispersion * (1 - hatvalues(model)))
resid( model, type="pearson" )
library(GLMsData)
data(trees)
cherry.m1 <- glm( Volume ~ log(Girth) + log(Height),
family=Gamma(link=log), data=trees)
coef( cherry.m1 )
# Provides qresid()
rP <- resid( cherry.m1, type="pearson" )
rD <- resid( cherry.m1 ) # Deviance resids are the default
rQ <- qresid( cherry.m1 )
phi.est <- summary( cherry.m1 )$dispersion # Pearson estimate
rP.std <- rP / sqrt( phi.est*(1 - hatvalues(cherry.m1)) )
rP.std
data(trees)
trees
trees$Volume <- round(trees$Volume)
cherry.m1 <- glm( Volume ~ log(Girth) + log(Height),
family=quasipoisson(link="log"), data=trees)
coef( cherry.m1 )
# Provides qresid()
rP <- resid( cherry.m1, type="pearson" )
rP <- resid( cherry.m1, type="pearson" )
phi.est <- summary( cherry.m1 )$dispersion # Pearson estimate
phi.est
rP.std <- rP / sqrt( phi.est*(1 - hatvalues(cherry.m1)) )
rP.std <- rP / sqrt( phi.est*(1 - hatvalues(cherry.m1)) )
rP.std
rP <- resid( cherry.m1, type="pearson" )
phi.est <- summary( cherry.m1 )$dispersion # Pearson estimate
rP.std <- rP / sqrt( phi.est*(1 - hatvalues(cherry.m1)) )
rP
phi.est
rP.std
model <- glm(value ~ origin + dev + 0, family = quasipoisson(link = "log"), data = data)
resid( model, type="pearson" )
summary(model)$dispersion
(1 - hatvalues(model)
resid( model, type="pearson" ) / sqrt(summary(model)$dispersion * (1 - hatvalues(model)))
resid( model, type="pearson" )
sqrt(summary(model)$dispersion * (1 - hatvalues(model)))
resid( model, type="pearson" ) / sqrt(summary(model)$dispersion * (1 - hatvalues(model)))
sqrt(summary(model)$dispersion)
library(keras)
mnist <- dataset_fashion_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
x_train <- array_reshape(x_train, c(nrow(x_train), 28 * 28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28 * 28))
x_train <- x_train / 255
x_test <- x_test / 255
x_train <- scale(x_train)
x_test <- scale(x_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 64, kernel_size = c(9, 9), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_batch_normalization() %>%
layer_conv_2d(filters = 128, kernel_size = c(7, 7), activation = "relu") %>%
layer_batch_normalization() %>%
layer_conv_2d(filters = 256, kernel_size = c(5, 5), activation = "relu") %>%
layer_batch_normalization() %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d() %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = .0001),
metrics = c("accuracy")
)
history <- model %>% fit(
x_train,
y_train,
epochs = 15,
batch_size = 256,
validation_split = 0.2
)
model %>% evaluate(x_test, y_test)
pred <- model %>% predict_classes(x_test)
q()
install.packages("learnr")
library(tidyverse)
library(caret)
library(dataPreparation)
library(ROCR)
library(xgboost)
library(randomForest)
rm(list = ls())
setwd("/home/marco/Documents/gitrepos/fraud_detection/MDV")
train_data <- read_csv("../data/train_final.csv") %>%
mutate_if(is.character, as.factor) %>%
mutate(FraudFound = as.factor(FraudFound)) %>%
rename(y = FraudFound)
test_data <- read_csv("../data/test_final.csv") %>%
mutate_if(is.character, as.factor) %>%
mutate(FraudFound = as.factor(FraudFound)) %>%
rename(y = FraudFound)
map(train_data, ~sum(is.na(.)))
map(test_data, ~sum(is.na(.)))
# scale
mean_sd_scale <- build_scales(train_data, verbose = FALSE)
train_data <- fastScale(train_data, scales = mean_sd_scale, verbose = FALSE)
test_data <- fastScale(test_data, scales = mean_sd_scale, verbose = FALSE)
library(randomForest)
train_data
rf <- randomForest(y ~ ., data = train_data)
rf_pred = predict(rf, newdata = test_data)
rf_pred
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y - 1))$overall["Accuracy"])
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(rf_pred, test_data$y)
rf_roc_data <- prediction(rf_pred, test_data$y - 1)
rf_pred
rf_roc_data <- prediction(as,numeric(rf_pred), test_data$y - 1)
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y - 1)
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y)
rf_pred = predict(rf, newdata = test_data)
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y, importance = TRUE)
rf_pred = predict(rf, newdata = test_data)
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
rf
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y,
importance = TRUE, ntree = 500, mtry = 6)
rf
train_data[-"y"]
train_data["y"]
train_data
class(train_data)
train_data[-c("y")]
select(train_data, y)
select(train_data, -y)
rf.cv <- rfcv(select(train_data, -y), train)data$y, cv.fold=10)
rf.cv <- rfcv(select(train_data, -y), train_data$y, cv.fold=10)
rf.cv
with(rf.cv, plot(n.var, error.cv, log="x", type="o", lwd=2))
which.min(rf.cv$error.cv)
rf.cv$n.var[which.min(rf.cv$error.cv)]
sum(train_data$y == 1)
sum(train_data$y == 0)
sum(train_data$y == 1)
class(train_data$y)
levels(train_data$y)
sum(train_data$y == 0) / sum(train_data$y == 1)
prop_sample <- sum(train_data$y == 0) / sum(train_data$y == 1)
prop_sample
sum(train_data$y == 0)
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y, sampsize = c("0" = 50 * prop_sample, "1" = 50),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y, sampsize = c("0" = 10 * prop_sample, "1" = 10),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf <- randomForest(y ~ ., data = train_data, strata = train_data$y, sampsize = c("0" = 1 * prop_sample, "1" = 1),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
round(sum(train_data$y == 0)
sum(train_data$y == 0)
sum(train_data$y == 1)
1/sum(train_data$y == 0)
1/sum(train_data$y == 1)
rf <- randomForest(y ~ ., data = train_data, classwt = c("0" = 1/sum(train_data$y == 0) , "1" = 1/sum(train_data$y == 1)),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf_pred = predict(rf, newdata = test_data)
rf_pred
hist(rf_pred)
hist(as.numeric(rf_pred))
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
sum(train_data$y == 0)/length(train_data$y)
sum(train_data$y == 1)/length(train_data$y)
rf <- randomForest(y ~ ., data = train_data,
classwt = c("0" = sum(train_data$y == 0)/length(train_data$y), "1" = length(train_data$y)),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf_pred = predict(rf, newdata = test_data)
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
rf <- randomForest(y ~ ., data = train_data,
classwt = c("0" = sum(train_data$y == 1)/length(train_data$y), "1" = length(train_data$y)),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf_pred = predict(rf, newdata = test_data)
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
rf <- randomForest(y ~ ., data = train_data, classwt = c("0" = 1/sum(train_data$y == 0), "1" = 1/sum(train_data$y == 1)),
importance = TRUE, ntree = 500, mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf.cv <- rfcv(select(train_data, -y), train_data$y,
cv.fold=10,
classwt = c("0" = 1/sum(train_data$y == 0), "1" = 1/sum(train_data$y == 1)))
1/sum(train_data$y == 0)/ (1/sum(train_data$y == 1) + 1/sum(train_data$y == 0))
1/sum(train_data$y == 1)/ (1/sum(train_data$y == 1) + 1/sum(train_data$y == 0))
sum(train_data$y == 0)
1/sum(train_data$y == 0)
1/sum(train_data$y == 1))
1/sum(train_data$y == 1)
table(train_data$y)
1/sum(train_data$y == 0)/ (1/sum(train_data$y == 1) + 1/sum(train_data$y == 0))
1/sum(train_data$y == 1)/ (1/sum(train_data$y == 1) + 1/sum(train_data$y == 0))
8176/(8176+496)
sum(train_data$y == 0)/length(train_data$y)
rf.cv <- rfcv(select(train_data, -y), train_data$y,
cv.fold=10,
classwt = c("0" = sum(train_data$y == 1)/length(train_data$y), "1" = sum(train_data$y == 0)/length(train_data$y)))
rf <- randomForest(y ~ .,
data = train_data,
classwt = c("0" = sum(train_data$y == 1)/length(train_data$y), "1" = sum(train_data$y == 0)/length(train_data$y)),
mtry = rf.cv$n.var[which.min(rf.cv$error.cv)])
rf_pred = predict(rf, newdata = test_data)
(rf_acc <- confusionMatrix(as.factor(rf_pred), as.factor(test_data$y))$overall["Accuracy"])
rf_roc_data <- prediction(as.numeric(rf_pred), test_data$y)
(rf_auc <- performance(rf_roc_data, measure = "auc")@y.values[[1]])
plot(rf_auc)
performance(rf_roc_data, measure = "auc")
plot(performance(rf_roc_data, measure = "auc"))
